{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(length, idx):\n",
    "    encode = np.zeros(shape=[length])\n",
    "    encode[idx] = 1.\n",
    "    return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_states = env.env.nS\n",
    "n_actions = env.env.nA\n",
    "print(f'{n_states:,} states & {n_actions:,} actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# inputs & targets (states & actions)\n",
    "inputs = tf.placeholder(tf.float32, shape=[n_states])\n",
    "target = tf.placeholder(tf.float32, shape=[1, n_actions])\n",
    "\n",
    "# reshape\n",
    "X_reshape = tf.reshape(inputs, shape=[1, n_states])\n",
    "\n",
    "# weights\n",
    "weight = tf.Variable(tf.random_normal(shape=[n_states, n_actions], mean=0, stddev=0.4))\n",
    "\n",
    "# Q value prediction\n",
    "Q_value = tf.matmul(X_reshape, weight)\n",
    "predict = tf.argmax(Q_value, axis=1)\n",
    "print(Q_value, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.squared_difference(target, Q_value)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-1)\n",
    "train = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "episodes = 10000\n",
    "max_trans_per_episode = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state, done = env.reset(), False\n",
    "    total_reward = 0\n",
    "    max_trans = 0\n",
    "    while max_trans < max_trans_per_episode:\n",
    "        max_trans += 1\n",
    "        action, Q = sess.run([predict, Q_value], \n",
    "                             feed_dict={inputs: one_hot(n_states, state)})\n",
    "        # Epsilon Greedy Exploration\n",
    "        if np.random.randn(1) < epsilon:\n",
    "            action[0] = env.action_space.sample()\n",
    "        # Take the action\n",
    "        new_state, reward, done, _ = env.step(action[0])\n",
    "        # Get QÂ´ values for the next_state\n",
    "        new_Q = sess.run(Q_value, feed_dict={inputs: one_hot(n_states, new_state)})\n",
    "        Q[0, action[0]] = reward + gamma * np.max(new_Q)\n",
    "        # Train network\n",
    "        sess.run(train, feed_dict={inputs: one_hot(n_states, state), target: Q})\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "    if episode % 100 == 0:\n",
    "        print(f'Episode: {episode}\\tTotal reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
