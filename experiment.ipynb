{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_name = 'Pong-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: (210, 160, 3)\tActions: 6\n"
     ]
    }
   ],
   "source": [
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "print(f'States: {state_shape}\\tActions: {n_actions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def network(state):\n",
    "    net = tf.reshape(state, [1, *state_shape])\n",
    "    # 3 convolutional layers\n",
    "    net = tf.layers.conv2d(net, filters=16, kernel_size=5, strides=1, padding='same')\n",
    "    net = tf.layers.conv2d(net, filters=16, kernel_size=5, strides=2, padding='same')\n",
    "    net = tf.layers.conv2d(net, filters=32, kernel_size=5, strides=2, padding='same')\n",
    "    # flattening layer\n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    # 2 fully connected layers\n",
    "    net = tf.layers.dense(net, units=128, activation=tf.nn.relu)\n",
    "    Q_value = tf.layers.dense(net, units=n_actions)\n",
    "    predict = tf.argmax(Q_value, axis=1)\n",
    "    return Q_value, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "state_placeholder = tf.placeholder(tf.float32, shape=state_shape)\n",
    "action_paceholder = tf.placeholder(tf.int32, shape=[1, n_actions])\n",
    "\n",
    "# Loss function\n",
    "Q_value, predict = network(state_placeholder)\n",
    "x_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Q_value, \n",
    "                                                    labels=action_paceholder)\n",
    "loss = tf.reduce_mean(x_entropy)\n",
    "\n",
    "# Training\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesnorflow's `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_dir = f'saved/experiment/{env_name}'\n",
    "tboard_dir = os.path.join(save_dir, 'tensorboard')\n",
    "logdir = os.path.join(tboard_dir, 'log')\n",
    "\n",
    "model_dir = os.path.join(save_dir, 'models')\n",
    "model_path = os.path.join(model_dir, 'model.ckpt')\n",
    "\n",
    "# Summary\n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Saver & Writer\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(logdir=logdir, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Attempting to restore last checkpoint\n",
      "INFO:tensorflow:Restoring parameters from saved/experiment/Pong-v0/models/model.ckpt-600\n",
      "INFO: Successfully restored last chekcpoint - saved/experiment/Pong-v0/models/model.ckpt-600\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(model_dir):\n",
    "    try:\n",
    "        print('INFO: Attempting to restore last checkpoint')\n",
    "        ckpt_file = tf.train.latest_checkpoint(model_dir)\n",
    "        saver.restore(sess=sess, save_path=ckpt_file)\n",
    "        print(f'INFO: Successfully restored last chekcpoint - {ckpt_file}')\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f'ERR: Could not restore checkpoint. {e}')\n",
    "        sys.stderr.flush()\n",
    "else:\n",
    "    tf.gfile.MakeDirs(model_dir)\n",
    "    print(f'INFO: Created checkpoint directory - {model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Play the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(episodes, **kwargs):\n",
    "    # Keyword arguments\n",
    "    gamma = kwargs.get('gamma', 0.9)\n",
    "    render = kwargs.get('render', False)\n",
    "    logging = kwargs.get('logging', True)\n",
    "    save_step = kwargs.get('save_step', 100)\n",
    "    max_trans_per_episode = kwargs.get('max_trans_per_episode', 200)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'wins':    0,\n",
    "        'rewards': 0,\n",
    "    }\n",
    "    # Game loop\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        max_trans = 0\n",
    "        \n",
    "        while max_trans < max_trans_per_episode:\n",
    "            max_trans += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "            # Room for E-greedy exploration\n",
    "            Q, _predict = sess.run([Q_value, predict], \n",
    "                                         feed_dict={state_placeholder: state})\n",
    "            # !- Random exploration\n",
    "            action = _predict[0]\n",
    "            \n",
    "            # Transition to a new state\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Get the next Q value\n",
    "            _next_Q = sess.run([Q_value], feed_dict={state_placeholder: new_state})\n",
    "            Q[0, action] = reward + gamma * np.max(_next_Q)\n",
    "            \n",
    "            # Train\n",
    "            feed_dict = {state_placeholder: state, action_paceholder: Q}\n",
    "            _, i_global = sess.run([train, global_step], feed_dict=feed_dict)\n",
    "            \n",
    "            # Update parameters\n",
    "            metrics['rewards'] += reward\n",
    "            state = new_state\n",
    "            \n",
    "            # Logging\n",
    "            if logging:\n",
    "                sys.stdout.write(f'\\rEpisode: {episode+1:,}\\tGlobal steps: {i_global:,}'\n",
    "                                 f'\\tReward: {reward}')\n",
    "                sys.stdout.flush()\n",
    "            # Game won!\n",
    "            if done:\n",
    "                metrics['wins'] += 1\n",
    "                print(f'\\n{60*\"=\"}')\n",
    "                print('\\t\\tTraining metrics - {}'.format(metrics))\n",
    "                print(f'{60*\"=\"}')\n",
    "                break\n",
    "            \n",
    "        # Save model at intervals\n",
    "        if episode % save_step == 0:\n",
    "            saver.save(sess=sess, save_path=model_path, global_step=global_step)\n",
    "            summary = sess.run(merged, feed_dict=feed_dict)\n",
    "            writer.add_summary(summary=summary, global_step=i_global)\n",
    "            print(f'\\n{60*\"-\"}')  # Break print overriding\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\tGlobal steps: 692\tReward: 0.00"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-e', '--episodes', default=10000,\n",
    "                        help='How many episodes should this game last?')\n",
    "    parser.add_argument('-r', '--render', default=True,\n",
    "                        help='Show game window. This may be slow depending on' \\\n",
    "                              + ' your machine')\n",
    "    parser.add_argument('-g', '--gamma', default=0.9,\n",
    "                        help='Markov Decision Process discount factor')\n",
    "    parser.add_argument('-s', '--save_step', default=100,\n",
    "                        help='Interval to save the trained model so far.')\n",
    "    parser.add_argument('-l', '--logging', default=True,\n",
    "                        help='Show training metrics')\n",
    "    parser.add_argument('-m', '--max_trans_per_episode', default=200,\n",
    "                        help='Max transitions per episode')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Start the game!\n",
    "    run_episode(**vars(args))\n",
    "    \"\"\"\n",
    "    run_episode(episodes=10000, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
